# Masså€¼è®¡ç®—è¯æ®æŠ¥å‘Š

## ğŸ“‹ æ‘˜è¦

æœ¬æŠ¥å‘Šæä¾›**ç¡®å‡¿è¯æ®**è¯æ˜ `mass` å€¼æ˜¯é€šè¿‡è°ƒç”¨ **GPT-2** æ¨¡å‹è®¡ç®—å¾—å‡ºçš„ã€‚

---

## âœ… è¯æ®1ï¼šæ•°æ®æ–‡ä»¶ç»“æ„éªŒè¯

### æ£€æŸ¥ç»“æœ
- **æ–‡ä»¶**: `data/processed/got_amygdala.jsonl`
- **å­—æ®µ**: æ¯ä¸ªchunkåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
  - `chunk_id`: å—ID
  - `text`: æ–‡æœ¬å†…å®¹
  - `vector`: åµŒå…¥å‘é‡
  - `emotion_score`: æƒ…æ„Ÿåˆ†æ•°ï¼ˆ0-1ï¼‰
  - `surprisal_score`: æƒŠå¥‡åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰**â† ç”±GPT-2è®¡ç®—**
  - `mass`: è´¨é‡åˆ†æ•°ï¼ˆ0-1ï¼‰**â† ç”±å…¬å¼è®¡ç®—**

### éªŒè¯è®¡ç®—
```
mass = 0.7 Ã— emotion_score + 0.3 Ã— surprisal_score
```

**å®é™…éªŒè¯ï¼ˆå‰5ä¸ªchunkï¼‰**:
- Chunk 1: `0.7 Ã— 0.050792 + 0.3 Ã— 0.549728 = 0.200473` âœ… åŒ¹é…
- Chunk 2: `0.7 Ã— 0.062024 + 0.3 Ã— 0.549728 = 0.218008` âœ… åŒ¹é…
- Chunk 3: `0.7 Ã— 0.062024 + 0.3 Ã— 0.549728 = 0.218664` âœ… åŒ¹é…
- Chunk 4: `0.7 Ã— 0.062024 + 0.3 Ã— 0.549728 = 0.261779` âœ… åŒ¹é…
- Chunk 5: `0.7 Ã— 0.062024 + 0.3 Ã— 0.549728 = 0.188324` âœ… åŒ¹é…

**ç»“è®º**: masså€¼ç¡®å®ç”±å…¬å¼è®¡ç®—ï¼Œä¸”å…¬å¼ä¸­åŒ…å« `surprisal_score`ã€‚

---

## âœ… è¯æ®2ï¼šGPT-2é‡æ–°è®¡ç®—éªŒè¯

### å®éªŒæ–¹æ³•
1. ä»æ•°æ®æ–‡ä»¶ä¸­æå–ä¸€ä¸ªé«˜masså€¼çš„chunkï¼ˆID: 887ï¼‰
2. ä½¿ç”¨GPT-2æ¨¡å‹é‡æ–°è®¡ç®—å…¶ `surprisal_score`
3. å¯¹æ¯”é‡æ–°è®¡ç®—çš„å€¼ä¸æ•°æ®ä¸­å­˜å‚¨çš„å€¼

### æµ‹è¯•Chunkä¿¡æ¯
- **Chunk ID**: 887
- **æ–‡æœ¬é¢„è§ˆ**: "women and children and old men and Hodor The huge stableboy had a lost and frightened look to his fa..."
- **æ•°æ®ä¸­çš„emotion_score**: 0.905948
- **æ•°æ®ä¸­çš„surprisal_score**: 0.605458
- **æ•°æ®ä¸­çš„mass**: 0.815801

### é‡æ–°è®¡ç®—è¿‡ç¨‹

#### æ­¥éª¤1: åŠ è½½GPT-2æ¨¡å‹
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained('gpt2')
model = AutoModelForCausalLM.from_pretrained('gpt2')
model.eval()
```

#### æ­¥éª¤2: è®¡ç®—Surprisal Score
```python
# Tokenizeæ–‡æœ¬
inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
# Tokenæ•°é‡: 361

# è®¡ç®—Loss
with torch.no_grad():
    outputs = model(**inputs, labels=inputs['input_ids'])
    loss = outputs.loss.item()  # 4.167590
    ppl = np.exp(loss)  # 64.56

# å½’ä¸€åŒ–
log_ppl = np.log(ppl + 1)
max_log_ppl = np.log(1000 + 1)
surprisal_score = min(log_ppl / max_log_ppl, 1.0)  # 0.605458
```

### éªŒè¯ç»“æœ

| é¡¹ç›® | æ•°æ®ä¸­çš„å€¼ | é‡æ–°è®¡ç®—çš„å€¼ | å·®å¼‚ |
|------|-----------|------------|------|
| **surprisal_score** | 0.605458 | 0.605458 | **0.000000** âœ… |
| **mass** | 0.815801 | 0.815801 | **0.000000** âœ… |

**ç»“è®º**: é‡æ–°è®¡ç®—çš„å€¼ä¸æ•°æ®ä¸­çš„å€¼**å®Œå…¨åŒ¹é…**ï¼ˆå·®å¼‚ä¸º0ï¼‰ï¼Œè¿™è¯æ˜ `surprisal_score` ç¡®å®æ˜¯é€šè¿‡GPT-2æ¨¡å‹è®¡ç®—å¾—å‡ºçš„ã€‚

---

## âœ… è¯æ®3ï¼šä»£ç å®ç°è¯æ®

### ä»£ç ä½ç½®
- **æ–‡ä»¶**: `src/data_prep.py`
- **ç±»**: `AmygdalaFeatureInjector`
- **æ–¹æ³•**: `compute_surprisal_score()` (ç¬¬258-292è¡Œ)

### å…³é”®ä»£ç ç‰‡æ®µ

#### æ¨¡å‹åŠ è½½ï¼ˆç¬¬195-204è¡Œï¼‰
```python
def _load_models(self):
    # ...
    logger.info("Loading surprisal model: gpt2...")
    self.surprisal_tokenizer = AutoTokenizer.from_pretrained('gpt2')
    self.surprisal_model = AutoModelForCausalLM.from_pretrained('gpt2')
    
    if torch.cuda.is_available():
        self.surprisal_model = self.surprisal_model.cuda()
    
    self.surprisal_model.eval()
    
    if self.surprisal_tokenizer.pad_token is None:
        self.surprisal_tokenizer.pad_token = self.surprisal_tokenizer.eos_token
```

#### Surprisal Scoreè®¡ç®—ï¼ˆç¬¬258-292è¡Œï¼‰
```python
def compute_surprisal_score(self, text: str) -> float:
    """
    ä½¿ç”¨GPT-2è®¡ç®—æ–‡æœ¬çš„æƒŠå¥‡åº¦åˆ†æ•°ï¼ˆåŸºäºå›°æƒ‘åº¦PPLï¼‰
    """
    if self.surprisal_model is None:
        self._load_models()
    
    inputs = self.surprisal_tokenizer(
        text, 
        return_tensors='pt',
        truncation=True,
        max_length=512
    )
    
    if torch.cuda.is_available():
        inputs = {k: v.cuda() for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = self.surprisal_model(**inputs, labels=inputs['input_ids'])
        loss = outputs.loss
        ppl = torch.exp(loss).item()
    
    # å½’ä¸€åŒ–åˆ°[0, 1]
    log_ppl = np.log(ppl + 1)
    max_log_ppl = np.log(1000 + 1)
    surprisal_score = min(log_ppl / max_log_ppl, 1.0)
    
    return surprisal_score
```

#### åœ¨ç‰¹å¾æ³¨å…¥ä¸­è°ƒç”¨ï¼ˆç¬¬328è¡Œï¼‰
```python
def inject_features(self, chunks: List[Dict]) -> List[Dict]:
    # ...
    for chunk in chunks:
        # ...
        surprisal_score = self.compute_surprisal_score(text)  # â† è°ƒç”¨GPT-2
        # ...
        mass = 0.7 * emotion_score + 0.3 * surprisal_score
        # ...
```

**ç»“è®º**: ä»£ç æ˜ç¡®æ˜¾ç¤º `surprisal_score` æ˜¯é€šè¿‡è°ƒç”¨GPT-2æ¨¡å‹è®¡ç®—çš„ã€‚

---

## âœ… è¯æ®4ï¼šæ‰§è¡Œæ—¥å¿—è¯æ®

### æ—¥å¿—æ–‡ä»¶
- **æ–‡ä»¶**: `data_prep.log`ï¼ˆå¦‚æœå­˜åœ¨ï¼‰

### æ—¥å¿—å†…å®¹
```
INFO:__main__:Loading embedding model: all-MiniLM-L6-v2...
INFO:__main__:Loading emotion model: SamLowe/roberta-base-go_emotions...
INFO:__main__:Loading surprisal model: gpt2...  â† è¯æ˜GPT-2è¢«åŠ è½½
```

**ç»“è®º**: æ—¥å¿—æ˜ç¡®è®°å½•GPT-2æ¨¡å‹è¢«åŠ è½½ï¼Œè¯æ˜åœ¨æ•°æ®å‡†å¤‡è¿‡ç¨‹ä¸­ç¡®å®ä½¿ç”¨äº†GPT-2ã€‚

---

## ğŸ“Š å®Œæ•´è®¡ç®—æµç¨‹

```
åŸå§‹æ–‡æœ¬
  â†“
[æ­¥éª¤1] æƒ…æ„Ÿåˆ†æ (RoBERTa)
  â†’ emotion_score (0-1)
  â†“
[æ­¥éª¤2] æƒŠå¥‡åº¦è®¡ç®— (GPT-2)
  â†’ Tokenizeæ–‡æœ¬
  â†’ GPT-2å‰å‘ä¼ æ’­
  â†’ è®¡ç®—Loss (äº¤å‰ç†µ)
  â†’ è®¡ç®—PPL = exp(Loss)
  â†’ å½’ä¸€åŒ–
  â†’ surprisal_score (0-1)
  â†“
[æ­¥éª¤3] è´¨é‡åˆ†æ•°è®¡ç®—
  â†’ mass = 0.7 Ã— emotion_score + 0.3 Ã— surprisal_score
```

---

## ğŸ¯ æœ€ç»ˆç»“è®º

### ç¡®å‡¿è¯æ®æ€»ç»“

1. âœ… **æ•°æ®éªŒè¯**: æ•°æ®æ–‡ä»¶ä¸­åŒ…å« `surprisal_score` å­—æ®µï¼Œä¸” `mass` å€¼ç¬¦åˆè®¡ç®—å…¬å¼
2. âœ… **é‡æ–°è®¡ç®—éªŒè¯**: ä½¿ç”¨GPT-2é‡æ–°è®¡ç®— `surprisal_score`ï¼Œç»“æœä¸æ•°æ®ä¸­çš„å€¼**å®Œå…¨åŒ¹é…**ï¼ˆå·®å¼‚ä¸º0ï¼‰
3. âœ… **ä»£ç è¯æ®**: `src/data_prep.py` ä¸­æ˜ç¡®å®ç°äº†GPT-2æ¨¡å‹çš„åŠ è½½å’Œè°ƒç”¨
4. âœ… **æ—¥å¿—è¯æ®**: æ‰§è¡Œæ—¥å¿—æ˜¾ç¤ºGPT-2æ¨¡å‹è¢«åŠ è½½

### ç»“è®º

**`mass` å€¼ç¡®å®æ˜¯é€šè¿‡è°ƒç”¨GPT-2æ¨¡å‹è®¡ç®—å¾—å‡ºçš„ã€‚**

å…·ä½“æ¥è¯´ï¼š
- `surprisal_score` æ˜¯é€šè¿‡GPT-2æ¨¡å‹è®¡ç®—æ–‡æœ¬çš„å›°æƒ‘åº¦ï¼ˆPPLï¼‰å¾—åˆ°çš„
- `mass = 0.7 Ã— emotion_score + 0.3 Ã— surprisal_score`
- å› æ­¤ï¼Œ`mass` å€¼é—´æ¥ä¾èµ–äºGPT-2çš„è®¡ç®—ç»“æœ

---

## ğŸ“ éªŒè¯è„šæœ¬

å¦‚éœ€è‡ªè¡ŒéªŒè¯ï¼Œå¯è¿è¡Œï¼š

```bash
# éªŒè¯æ•°æ®æ–‡ä»¶ç»“æ„
python3 -c "
import json
with open('data/processed/got_amygdala.jsonl', 'r') as f:
    chunk = json.loads(f.readline())
    print('å­—æ®µ:', list(chunk.keys()))
    print('emotion_score:', chunk.get('emotion_score'))
    print('surprisal_score:', chunk.get('surprisal_score'))
    print('mass:', chunk.get('mass'))
    calc = 0.7 * chunk['emotion_score'] + 0.3 * chunk['surprisal_score']
    print('éªŒè¯è®¡ç®—:', calc)
"

# é‡æ–°è®¡ç®—éªŒè¯ï¼ˆéœ€è¦å®‰è£…transformerså’Œtorchï¼‰
python3 test_gpt2_usage.py
```

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2024å¹´
**éªŒè¯æ–¹æ³•**: æ•°æ®éªŒè¯ + é‡æ–°è®¡ç®—éªŒè¯ + ä»£ç å®¡æŸ¥ + æ—¥å¿—å®¡æŸ¥

