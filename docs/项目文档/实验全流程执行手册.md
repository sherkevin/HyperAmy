# ğŸ›¡ï¸ HyperAmy å®éªŒå…¨æµç¨‹æ‰§è¡Œæ‰‹å†Œ (GoT ç‰ˆ)

## 0. æ³•å¾‹ä¸åˆè§„æ€§å£°æ˜ (Copyright Strategy)

åš NLP å®éªŒï¼Œ**ç‰ˆæƒ (Copyright)** æ˜¯ç»•ä¸å¼€çš„å‘ã€‚

- **çº¢çº¿ï¼š** ä¸¥ç¦å°†ã€ŠæƒåŠ›çš„æ¸¸æˆã€‹å…¨æ–‡ `.txt` ç›´æ¥ä¸Šä¼ åˆ° GitHub å…¬å¼€ä»“åº“ã€‚è¿™å¿…æ­»æ— ç–‘ã€‚
- **ç»¿ç¯ï¼ˆFair Useï¼‰ï¼š**
  1. **æœ¬åœ°ä½¿ç”¨ï¼š** ä½ è‡ªå·±åœ¨æœ¬åœ°è·‘å®éªŒï¼Œä¸åˆ†å‘ä¹¦ç±åŸæ–‡ï¼Œå®Œå…¨åˆæ³•ã€‚
  2. **ä»£ç è„±æ•ï¼š** ä¸Šä¼ ä»£ç æ—¶ï¼Œ`data/` æ–‡ä»¶å¤¹é‡Œåªèƒ½æ”¾ `sample.txt` (æ¯”å¦‚åªæœ‰ç¬¬ä¸€ç« )ï¼Œ**å…¨ä¹¦å¿…é¡»æ”¾å…¥ `.gitignore`**ã€‚
  3. **å¼•ç”¨åˆè§„ï¼š** åœ¨è®ºæ–‡ä¸­å¼•ç”¨æ—¶ï¼Œä¸è¦å¤§æ®µæ‘˜æŠ„åŸæ–‡ï¼ˆè¶…è¿‡ 200 è¯ï¼‰ï¼Œåªå¼•ç”¨å¿…è¦çš„å¥å­ã€‚

**âœ… æ“ä½œæŒ‡ä»¤ï¼š** åœ¨é¡¹ç›®æ ¹ç›®å½•æ–°å»º `.gitignore` æ–‡ä»¶ï¼Œå†™å…¥ï¼š

Plaintext

```
data/books/*.txt
data/processed/*.jsonl
data/indexes/*
```

------

## 1. æ•°æ®è·å–ä¸é¢„å¤„ç† (Data Acquisition & Cleaning)

**ç›®æ ‡ï¼š** è·å¾—å¹²å‡€ã€åˆ†å—ã€å¸¦æƒ…æ„Ÿæ ‡è®°çš„ JSONL æ•°æ®ã€‚

### 1.1 è·å–ä¹¦ç±

- **æ¸ é“ï¼š** è´­ä¹°æ­£ç‰ˆ Kindle ç”µå­ä¹¦ï¼Œä½¿ç”¨ Calibre è½¯ä»¶è½¬æ¢ä¸º `.txt`ã€‚æˆ–è€…åœ¨ HuggingFace Datasets æœç´¢ `bookcorpus` (è™½æœ‰äº‰è®®ä½†å­¦æœ¯ç•Œå¸¸ç”¨)ã€‚
- **ä¸ºäº†å®éªŒæ–¹ä¾¿ï¼š** å‡è®¾ä½ å·²ç»æåˆ°äº† `got_book1.txt`ã€‚

### 1.2 æ™ºèƒ½åˆ†å— (Semantic Chunking)

ä¸è¦æŒ‰å­—ç¬¦åˆ‡ï¼ˆæ¯”å¦‚æ¯ 500 å­—ä¸€åˆ€ï¼‰ï¼Œè¿™ä¼šæŠŠâ€œæ€äººâ€çš„æƒ…èŠ‚åˆ‡æ–­ã€‚æˆ‘ä»¬è¦æŒ‰**è‡ªç„¶æ®µè½**åˆå¹¶ã€‚

**ç»™ Cursor çš„æŒ‡ä»¤ (Step 1):**

Markdown

```
Create `src/data_prep.py`.
1. Read `data/books/got_book1.txt`.
2. Implement a `SemanticChunker`:
   - Split text by paragraphs (`\n\n`).
   - Group paragraphs until the chunk reaches ~300 words.
   - If a paragraph is too long, split by sentences.
   - Maintain a 50-word overlap between chunks to preserve context.
3. Save chunks to `data/processed/got_chunks.jsonl` with fields: `{"chunk_id": int, "text": str}`.
```

------

## 2. æä»æ ¸ç‰¹å¾æ³¨å…¥ (Amygdala Feature Injection)

è¿™æ˜¯æœ€æ ¸å¿ƒçš„ä¸€æ­¥ã€‚æˆ‘ä»¬éœ€è¦ç»™æ¯ä¸€æ®µæ–‡å­—æ‰“ä¸Š **z (è¯­ä¹‰åæ ‡)** å’Œ **m (æœ¬èƒ½è´¨é‡)**ã€‚

### 2.1 è®¡ç®— m (Mass/Surprisal)

æˆ‘ä»¬ç”¨ **â€œæƒŠå¥‡åº¦ (Loss)â€** + **â€œè´Ÿé¢æƒ…æ„Ÿ (Negative Emotion)â€** è”åˆå®šä¹‰ mã€‚

- **Surprisal:** ç”¨ GPT-2 æˆ– Llama-3-8B è®¡ç®—è¿™æ®µè¯çš„ PPL (å›°æƒ‘åº¦)ã€‚æƒ…èŠ‚è¶Šç¦»è°±ï¼ŒPPL è¶Šé«˜ã€‚
- **Emotion:** ç”¨ `roberta-base-go_emotions` æ£€æµ‹ `fear`, `surprise`, `anger`ã€‚

**ç»™ Cursor çš„æŒ‡ä»¤ (Step 2):**

Markdown

```
Update `src/data_prep.py`.
Requires: `torch`, `transformers`, `sentence-transformers`.

1. Load models:
   - Embedding: `all-MiniLM-L6-v2` (for vector z).
   - Emotion: `SamLowe/roberta-base-go_emotions` (fast & accurate).
   - Surprisal: `gpt2` (to calculate log-likelihood/loss).

2. Process each chunk in `got_chunks.jsonl`:
   - `vector`: Get 384-dim embedding.
   - `emotion_score`: Run emotion model. If label is 'fear', 'surprise', 'anger', or 'disgust', score = probability. Else score = 0.
   - `surprisal_score`: Calculate GPT2 loss for the text. Normalize to 0-1 range (min-max scaling over all chunks).
   - `mass` (m): Formula `m = 0.7 * emotion_score + 0.3 * surprisal_score`. Clip to [0, 1].

3. Save to `data/processed/got_amygdala.jsonl`.
```

------

## 3. æ„é€ â€œæœ¬èƒ½æµ‹è¯•é¢˜â€ (Construction of Instinct Dataset)

**é™·é˜±ï¼š** å¦‚æœé—®é¢˜æ˜¯â€œJon Snow çš„å†°åŸç‹¼å«ä»€ä¹ˆï¼Ÿâ€ï¼Œæ™®é€šæ£€ç´¢ä¹Ÿèƒ½åšå¯¹ã€‚ **ç›®æ ‡ï¼š** æ„é€ éœ€è¦**â€œæ„ŸçŸ¥å±æœºâ€**æ‰èƒ½å›ç­”çš„é—®é¢˜ã€‚

æˆ‘ä»¬ä½¿ç”¨ GPT-4o é€†å‘ç”Ÿæˆæ•°æ®é›†ã€‚

**ç»™ Cursor çš„æŒ‡ä»¤ (Step 3):**

Markdown

```
Create `src/gen_qa.py`.
Requires: OpenAI API Key.

1. Load `got_amygdala.jsonl`.
2. Filter for "High Mass Chunks" (where m > 0.8). These are high-tension moments (deaths, betrayals).
3. For each high-mass chunk, verify with GPT-4o:
   "Does this text contain a plot twist, danger signal, or hidden cue? If yes, generate a question that requires noticing this specific cue."
   
   Example format:
   - Chunk: "The musicians in the gallery began to play 'The Rains of Castamere'..."
   - Question: "What specific auditory cue signaled the start of the Red Wedding massacre to Catelyn Stark?"
   - Ground Truth: The lyrics of 'The Rains of Castamere'.

4. Generate 50 high-quality (Question, Answer, Chunk_ID) triplets.
5. Save to `data/benchmarks/instinct_qa.json`.
```

------

## 4. å®éªŒï¼šåŒç›²ç«æŠ€åœº (The Arena)

æˆ‘ä»¬è¦è·‘ä¸‰ç»„æ•°æ®ï¼Œå¹¶è®©è£åˆ¤æ‰“åˆ†ã€‚

### å®éªŒåˆ†ç»„

1. **Group A (Oracle):** ç›´æ¥æŠŠæ­£ç¡®ç­”æ¡ˆçš„ Chunk å–‚ç»™ LLMã€‚ä»£è¡¨**ç†è®ºä¸Šé™**ã€‚
2. **Group B (Baseline):** ç”¨ FAISS åšæ ‡å‡† Cosine æ£€ç´¢ã€‚ä»£è¡¨**ä¼ ç»Ÿ RAG**ã€‚
3. **Group C (HyperAmy):** ç”¨ä½ çš„**åºåŠ è±ç•¸å˜å…¬å¼**æ£€ç´¢ã€‚ä»£è¡¨**ä½ çš„æ–¹æ³•**ã€‚

**ç»™ Cursor çš„æŒ‡ä»¤ (Step 4):**

Markdown

```
Create `src/run_experiment.py`.

1. **Index Building:**
   - Load `got_amygdala.jsonl`.
   - Build a standard Index (List of vectors).

2. **Retrieval Functions:**
   - `retrieve_baseline(query_vec)`: Sort by `cosine_similarity(q, k)`.
   - `retrieve_hyperamy(query_vec, beta=10)`:
     - Compute `dist = cosine_dist(q, k)`.
     - Apply Warping: `warped_dist = dist / (1 + beta * k.mass)`.
     - Sort by `warped_dist`.

3. **Generation Loop:**
   - Iterate through `instinct_qa.json`.
   - For each Question:
     - Get Context_B (Top-3 from Baseline).
     - Get Context_C (Top-3 from HyperAmy).
     - Generate Answer_B using LLM + Context_B.
     - Generate Answer_C using LLM + Context_C.
   
4. **Save results** to `results/experiment_run_v1.json`.
```

------

## 5. è‡ªåŠ¨è¯„æµ‹ä¸èƒœç‡è®¡ç®— (The Judge)

ä¸è¦äººå·¥çœ‹ï¼Œå¤ªæ…¢ã€‚ç”¨ GPT-4o åšè£åˆ¤ã€‚

**ç»™ Cursor çš„æŒ‡ä»¤ (Step 5):**

Markdown

```
Create `src/evaluate.py`.

Prompt for GPT-4o-Judge:
"You are an expert literary critic.
Question: {question}
Ground Truth Fact: {oracle_fact}

Model B Answer: {ans_b}
Model C Answer: {ans_c}

Task: Compare Model B and C.
1. Did they retrieve the specific 'Ground Truth Fact'?
2. Which answer better captures the *imminence* or *emotional weight* of the event?
3. Output JSON: {'winner': 'B' or 'C' or 'Tie', 'reason': '...'}"

Run this for all 50 questions and calculate:
- HyperAmy Win Rate (Target: >60%)
- Retrieval Hit Rate (Target: C > B)
```

------

## 6. é¢„æœŸç»“æœä¸äº¤ä»˜ç‰© (Deliverables)

ä¸ºäº†è¯æ˜ä½ çš„å¥½ï¼Œä½ éœ€è¦æœŸå¾…çœ‹åˆ°ä»¥ä¸‹ç°è±¡ï¼š

1. **Case Studyï¼ˆå¿…æ€æŠ€ï¼‰ï¼š**
   - **Baseline** å¯èƒ½ä¼šæœåˆ°å¾ˆå¤šå…³äºâ€œæ­Œè¯â€ã€â€œéŸ³ä¹â€çš„é—²èŠç‰‡æ®µã€‚
   - **HyperAmy** ä¼šå› ä¸ºâ€œè¡€è‰²å©šç¤¼â€é‚£ä¸ªç‰‡æ®µçš„ m æé«˜ï¼ˆæƒŠå¥‡åº¦çˆ†è¡¨ï¼‰ï¼Œå³ä½¿ Query åªæ˜¯é—®â€œCatelyn å¬åˆ°äº†ä»€ä¹ˆâ€ï¼Œä¹Ÿèƒ½å¼ºè¡ŒæŠŠé‚£ä¸ªè¡€è…¥çš„å‰å¥ç‰‡æ®µæ‹‰åˆ° Top-1ã€‚
2. **æ•°æ®æ›²çº¿ï¼š**
   - å½“é—®é¢˜è¶Šå¹³æ·¡ï¼ˆLow Mass Queryï¼‰ï¼Œä¸¤è€…è¡¨ç°å·®ä¸å¤šã€‚
   - å½“é—®é¢˜è¶ŠæƒŠæ‚šï¼ˆHigh Mass Queryï¼‰ï¼ŒHyperAmy çš„ Recall æ˜¾è‘—é«˜äº Baselineã€‚**è¿™å¼ å›¾å°±æ˜¯ä½ è®ºæ–‡çš„æ ¸å¿ƒå›¾è¡¨ã€‚**

------

## 7. å¦‚ä½•è®© Cursor æ‰§è¡Œ (One-Click Prompt)

ä½ å¯ä»¥å¤åˆ¶ä¸‹é¢è¿™ä¸€æ•´æ®µï¼Œç›´æ¥æ‰”ç»™ Cursor çš„ Composer æ¨¡å¼ï¼Œå®ƒä¼šå¸®ä½ ç”Ÿæˆæ•´ä¸ªé¡¹ç›®ç»“æ„ï¼š

Plaintext

```
@workspace I need to implement the experiment pipeline for project "HyperAmy". 
The goal is to prove that "Affective-Modulated Retrieval" outperforms "Semantic Retrieval" on the book "A Game of Thrones".

Please scaffold the following project structure and implement the python scripts:

Project Structure:
- data/ (gitignored)
- src/
  - data_prep.py (Chunking, Embedding, Emotion/Surprisal Mass Calc)
  - gen_qa.py (Generate Instinct-based questions using LLM)
  - run_experiment.py (Compare Baseline vs HyperAmy retrieval)
  - evaluate.py (LLM-as-a-judge)
- requirements.txt

Key Implementation Details:
1. In `data_prep.py`: Use `all-MiniLM-L6-v2` for embeddings. Use `roberta-base-go_emotions` to calculate a 'mass' score (0-1) for each chunk. If emotion is fear/surprise, mass is high.
2. In `run_experiment.py`: Implement the HyperAmy retrieval formula: `effective_dist = cosine_dist / (1 + beta * mass)`. Set beta=10.
3. In `gen_qa.py`: Prompt the LLM to specifically generate questions about plot twists or danger signals that require high-mass contexts to answer.

Please start by creating the `requirements.txt` and `src/data_prep.py`.
```

è¿™ä¸€å¥—æµç¨‹ä¸‹æ¥ï¼Œå®Œå…¨åˆè§„ï¼Œé€»è¾‘é—­ç¯ï¼Œè€Œä¸”é’ˆå¯¹æ€§æå¼ºã€‚åªè¦ GoT çš„æ•°æ®ä¸€è·‘é€šï¼Œä½ å°±å¯ä»¥é©¬ä¸Šæ¢ã€Šç”„å¬›ä¼ ã€‹ï¼ˆåªéœ€æŠŠ Embedding æ¨¡å‹æ¢æˆæ”¯æŒä¸­æ–‡çš„ `text2vec-base-chinese` å³å¯ï¼Œæµç¨‹ä¸å˜ï¼‰ã€‚