#!/usr/bin/env python3
"""
çœŸå®æ¡ˆä¾‹æµ‹è¯•ï¼šåŸºç£å±±ä¼¯çˆµæ‹’ç»è‘¡è„å¹²åœºæ™¯ï¼ˆè¯¦ç»†æ—¥å¿—ç‰ˆæœ¬ï¼‰

æµ‹è¯•åœºæ™¯ï¼š
- Query: "Why did the Count strictly refuse the muscatel grapes and any refreshment
         offered by Madame de Morcerf (Mercedes) during his visit to her house?"
- å­˜å‚¨çš„Chunk: æ¥è‡ªã€ŠåŸºç£å±±ä¼¯çˆµã€‹çš„å‡ ä¸ªåœºæ™¯
- æ£€ç´¢æ¨¡å¼: chunk
- ç›®æ ‡: è¯¦ç»†å±•ç¤ºæ£€ç´¢è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç²’å­åˆ—è¡¨ã€æ˜ å°„å…³ç³»ã€å¾—åˆ†è®¡ç®—
"""
import sys
import logging
from typing import List, Dict, Any

# è®¾ç½®è¯¦ç»†çš„æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(levelname)s] [%(name)s] %(message)s'
)

from workflow.amygdala import Amygdala
from poincare.retrieval import HyperAmyRetrieval

print("=" * 100)
print("çœŸå®æ¡ˆä¾‹æµ‹è¯•ï¼šåŸºç£å±±ä¼¯çˆµ - æ‹’ç»è‘¡è„å¹²åœºæ™¯ï¼ˆè¯¦ç»†æ—¥å¿—ç‰ˆæœ¬ï¼‰")
print("=" * 100)

# æµ‹è¯•æ•°æ®ï¼šåŸºç£å±±ä¼¯çˆµçš„ç›¸å…³ç« èŠ‚
chunks = [
    # Chunk 1 - æ—©é¤åœºæ™¯
    '"I have an excellent appetite," said Albert. "I hope, my dear Count, you have the same." '
    '"I?" said Monte Cristo. "I never eat, or rather, I eat so little that it is not worth '
    'talking about. I have my own peculiar habits."',

    # Chunk 2 - è¯ä¸¸åœºæ™¯
    'The Count took from his pocket a small case made of hollowed emerald, took out a small '
    'greenish pill, and swallowed it. "This is my food," he said to the guests. "With this, '
    'I feel neither hunger nor fatigue. It is a secret I learned in the East."',

    # Chunk 3 - èŠ±å›­é‡Œçš„æ‹’ç»åœºæ™¯
    '"Will you not take anything?" asked Mercedes. "A peach? Some grapes?" '
    '"I thank you, Madame," replied Monte Cristo with a bow, "but I never eat between meals. '
    'It is a rule I have imposed upon myself to maintain my health."',

    # Chunk 5 - ä¸œæ–¹å“²å­¦ï¼ˆ20ç« å‰çš„å›å¿†ï¼‰
    '"In the countries of the East, where I have lived," said Monte Cristo to Franz, '
    '"people who eat and drink together are bound by a sacred tie. They become brothers. '
    'Therefore, I never eat or drink in the house of a man whom I wish to kill. '
    'If I shared their bread, I would be forbidden by honor to take my revenge."',

    # Chunk 6 - æƒ…æ„Ÿå¯¹å³™
    'Mercedes looked at him with terror in her eyes. Her hand trembled as she held the plate. '
    '"You refuse?" she whispered, her voice full of tears. "Is it because you are our enemy? '
    'To refuse to break bread... means you bring death to this house." '
    'She realized then that the man standing before her was not just a visitor, but an avenger '
    'who remembered the past.'
]

# åˆå§‹åŒ– Amygdala
print("\n" + "=" * 100)
print("ã€åˆå§‹åŒ–ã€‘åˆ›å»º Amygdala å®ä¾‹...")
print("=" * 100)

amygdala = Amygdala(
    save_dir="./test_monte_cristo_db",
    particle_collection_name="monte_cristo_particles",
    conversation_namespace="monte_cristo",
    embedding_model=None,
    auto_link_particles=False
)
print("âœ“ Amygdala åˆå§‹åŒ–å®Œæˆ")

# æ·»åŠ chunkåˆ°æ•°æ®åº“
print("\n" + "=" * 100)
print("ã€æ·»åŠ æ•°æ®ã€‘å°†ã€ŠåŸºç£å±±ä¼¯çˆµã€‹çš„ç« èŠ‚æ·»åŠ åˆ°æ•°æ®åº“...")
print("=" * 100)

chunk_ids = []
for i, chunk in enumerate(chunks, 1):
    print(f"\næ·»åŠ  Chunk {i}/{len(chunks)}:")
    print(f"  é¢„è§ˆ: {chunk[:80]}...")

    result = amygdala.add(chunk)
    chunk_ids.append(result['conversation_id'])
    print(f"  âœ“ ç”Ÿæˆäº† {result['particle_count']} ä¸ªç²’å­")
    print(f"  âœ“ Chunk ID: {result['conversation_id']}")

total_particles = sum(len(amygdala.get_particles_by_conversation(cid)) for cid in chunk_ids)
print(f"\nâœ“ æ€»å…±æ·»åŠ äº† {len(chunks)} ä¸ªchunkï¼Œç”Ÿæˆäº† {total_particles} ä¸ªç²’å­")

# æµ‹è¯•æŸ¥è¯¢
query = "Why did the Count strictly refuse the muscatel grapes and any refreshment offered by Madame de Morcerf (Mercedes) during his visit to her house?"

print("\n" + "=" * 100)
print("ã€æ£€ç´¢æµ‹è¯•ã€‘æ‰§è¡Œ Chunk æ¨¡å¼æ£€ç´¢ï¼ˆè¯¦ç»†æ—¥å¿—ï¼‰")
print("=" * 100)
print(f"\næŸ¥è¯¢é—®é¢˜: {query}")
print(f"æŸ¥è¯¢é•¿åº¦: {len(query)} å­—ç¬¦")

# ========== Step 1: æŸ¥è¯¢æ–‡æœ¬è½¬ç²’å­ ==========
print("\n" + "=" * 100)
print("ã€Step 1ã€‘å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºæŸ¥è¯¢ç²’å­")
print("=" * 100)

query_particles = amygdala.particle.process(
    text=query,
    text_id=f"query_detail"
)

print(f"\næŸ¥è¯¢æ–‡æœ¬ç”Ÿæˆäº† {len(query_particles)} ä¸ªç²’å­:")
for i, qp in enumerate(query_particles, 1):
    print(f"\n  æŸ¥è¯¢ç²’å­ {i}:")
    print(f"    - å®ä½“: {qp.entity}")
    print(f"    - ç²’å­ID: {qp.entity_id}")
    print(f"    - é€Ÿåº¦: {qp.speed:.4f}")
    print(f"    - æ¸©åº¦: {qp.temperature:.4f}")
    print(f"    - æƒé‡: {qp.weight:.4f}")
    print(f"    - å‘é‡ç»´åº¦: {len(qp.emotion_vector)}")

# ä½¿ç”¨ç¬¬ä¸€ä¸ªç²’å­ä½œä¸ºæŸ¥è¯¢ç²’å­
query_particle = query_particles[0]
print(f"\nâœ“ ä½¿ç”¨ '{query_particle.entity}' ä½œä¸ºä¸»æŸ¥è¯¢ç²’å­")

# ========== Step 2: ç²’å­æ£€ç´¢ ==========
print("\n" + "=" * 100)
print("ã€Step 2ã€‘æ‰§è¡Œç²’å­æ£€ç´¢ï¼ˆä½¿ç”¨æ··åˆæ£€ç´¢æµæ°´çº¿ï¼‰")
print("=" * 100)
print("\næ£€ç´¢å‚æ•°:")
print(f"  - æŸ¥è¯¢ç²’å­: {query_particle.entity}")
print(f"  - é”¥ä½“å®½åº¦ (cone_width): 50")
print(f"  - æœ€å¤§é‚»åŸŸ (max_neighbors): 20")
print(f"  - é‚»å±…æƒ©ç½š (neighbor_penalty): 1.1")

# åˆå§‹åŒ–æ£€ç´¢å™¨
retriever = HyperAmyRetrieval(
    storage=amygdala.particle_storage,
    projector=amygdala.particle_projector
)

# æ‰§è¡Œæ£€ç´¢
search_results = retriever.search(
    query_entity=query_particle,
    top_k=50,  # è·å–æ›´å¤šå€™é€‰ç²’å­
    cone_width=50,
    max_neighbors=20,
    neighbor_penalty=1.1
)

print(f"\nâœ“ æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç›¸å…³ç²’å­")
print(f"âœ“ è¯¦ç»†ä¿¡æ¯ï¼ˆTop 10ï¼‰:")

# æ˜¾ç¤ºTop 10ç²’å­
top_particles = search_results[:10]
for i, result in enumerate(top_particles, 1):
    # è·å–ç²’å­æ‰€å±çš„chunk
    chunk_id = amygdala.particle_to_conversation.get(result.id, "Unknown")

    # æ‰¾åˆ°chunkçš„æ–‡æœ¬ç‰‡æ®µ
    chunk_text = amygdala.get_conversation_text(chunk_id)
    chunk_preview = chunk_text[:60] + "..." if chunk_text and len(chunk_text) > 60 else chunk_text

    # ç¡®å®šchunkç±»å‹
    chunk_type = "Unknown"
    if chunk_id:
        if "In the countries of the East" in (chunk_text or ""):
            chunk_type = "ä¸œæ–¹å“²å­¦"
        elif "Mercedes looked at him with terror" in (chunk_text or ""):
            chunk_type = "æƒ…æ„Ÿå¯¹å³™"
        elif "Will you not take anything" in (chunk_text or ""):
            chunk_type = "æ‹’ç»è‘¡è„å¹²"
        elif "I have an excellent appetite" in (chunk_text or ""):
            chunk_type = "æ—©é¤åœºæ™¯"
        elif "The Count took from his pocket" in (chunk_text or ""):
            chunk_type = "è¯ä¸¸åœºæ™¯"

    print(f"\n  ç²’å­ {i}:")
    print(f"    - ç²’å­ID: {result.id}")
    print(f"    - å®ä½“åç§°: {result.metadata.get('entity', 'Unknown')}")
    print(f"    - åŒæ›²è·ç¦»: {result.score:.4f} (è¶Šå°è¶Šç›¸ä¼¼)")
    print(f"    - åŒ¹é…ç±»å‹: {result.match_type}")
    print(f"    - æ‰€å±Chunk: {chunk_type} (ID: {chunk_id[:40]}...)")
    print(f"    - Chunké¢„è§ˆ: {chunk_preview}")
    print(f"    - é€Ÿåº¦: {result.metadata.get('v', 0):.4f}")
    print(f"    - æ¸©åº¦: {result.metadata.get('T', 0):.4f}")
    print(f"    - æƒé‡: {result.metadata.get('weight', 1.0):.4f}")

# ========== Step 3: ç²’å­åˆ°Chunkæ˜ å°„ ==========
print("\n" + "=" * 100)
print("ã€Step 3ã€‘å°†ç²’å­æ˜ å°„åˆ°Chunkå¹¶è®¡ç®—å¾—åˆ†")
print("=" * 100)

print("\næ˜ å°„è§„åˆ™:")
print("  chunk_score = sum((total_particles - position) for each particle in chunk)")
print("  å…¶ä¸­ position æ˜¯ç²’å­åœ¨æœç´¢ç»“æœä¸­çš„ä½ç½®ï¼ˆ0-basedï¼Œè¶Šé å‰æƒé‡è¶Šå¤§ï¼‰")

# ç»Ÿè®¡æ¯ä¸ªchunkçš„å¾—åˆ†
from collections import defaultdict

chunk_data = defaultdict(lambda: {
    'score': 0,
    'particles': [],
    'particle_details': []
})

total_particles_found = len(search_results)

print(f"\nå¼€å§‹æ˜ å°„ {total_particles_found} ä¸ªç²’å­åˆ° {len(chunks)} ä¸ªchunk...")

for position, result in enumerate(search_results):
    particle_id = result.id
    chunk_id = amygdala.particle_to_conversation.get(particle_id)

    if not chunk_id:
        continue

    # è®¡ç®—è¯¥ç²’å­çš„æƒé‡è´¡çŒ®
    weight = (total_particles_found - position)

    # è·å–chunkæ–‡æœ¬ç”¨äºæ˜¾ç¤º
    chunk_text = amygdala.get_conversation_text(chunk_id)

    # ç¡®å®šchunkç±»å‹
    chunk_type = "Unknown"
    if chunk_id:
        if "In the countries of the East" in (chunk_text or ""):
            chunk_type = "ä¸œæ–¹å“²å­¦"
        elif "Mercedes looked at him with terror" in (chunk_text or ""):
            chunk_type = "æƒ…æ„Ÿå¯¹å³™"
        elif "Will you not take anything" in (chunk_text or ""):
            chunk_type = "æ‹’ç»è‘¡è„å¹²"
        elif "I have an excellent appetite" in (chunk_text or ""):
            chunk_type = "æ—©é¤åœºæ™¯"
        elif "The Count took from his pocket" in (chunk_text or ""):
            chunk_type = "è¯ä¸¸åœºæ™¯"

    # è®°å½•è¯¦ç»†ä¿¡æ¯
    chunk_data[chunk_id]['score'] += weight
    chunk_data[chunk_id]['particles'].append(particle_id)
    chunk_data[chunk_id]['particle_details'].append({
        'position': position,
        'weight': weight,
        'particle_id': particle_id,
        'entity': result.metadata.get('entity', 'Unknown'),
        'score': result.score,
        'chunk_type': chunk_type
    })

# æ˜¾ç¤ºè¯¦ç»†çš„æ˜ å°„è¿‡ç¨‹
print(f"\nâœ“ æ˜ å°„å®Œæˆï¼Œ{len(chunk_data)} ä¸ªchunkåŒ…å«ç›¸å…³ç²’å­\n")

# æŒ‰å¾—åˆ†æ’åº
sorted_chunks = sorted(
    chunk_data.items(),
    key=lambda x: x[1]['score'],
    reverse=True
)[:5]  # Top 5

for rank, (chunk_id, data) in enumerate(sorted_chunks, 1):
    chunk_text = amygdala.get_conversation_text(chunk_id)
    chunk_type = data['particle_details'][0]['chunk_type'] if data['particle_details'] else 'Unknown'

    print(f"\n{'=' * 100}")
    print(f"  Chunk {rank}: {chunk_type}")
    print(f"{'=' * 100}")
    print(f"\n  ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    print(f"    - Chunk ID: {chunk_id}")
    print(f"    - åŒ…å«ç²’å­æ•°: {len(data['particles'])}")
    print(f"    - æ€»å¾—åˆ†: {data['score']:.1f}")
    print(f"    - Chunkæ–‡æœ¬: {chunk_text}")

    print(f"\n  ğŸ“ å¾—åˆ†è®¡ç®—è¯¦æƒ…:")
    print(f"    'å¾—åˆ† = sum((æ€»ç²’å­æ•° - ä½ç½®) for each particle in chunk)'")
    print(f"    æ€»ç²’å­æ•° = {total_particles_found}")

    for i, detail in enumerate(data['particle_details'][:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"    - ç²’å­ {i}:")
        print(f"      * å®ä½“: {detail['entity']}")
        print(f"      * ä½ç½®: {detail['position']}")
        print(f"      * æƒé‡è´¡çŒ®: {total_particles_found} - {detail['position']} = {detail['weight']}")
        print(f"      * åŒæ›²è·ç¦»: {detail['score']:.4f}")

    if len(data['particle_details']) > 10:
        print(f"    - ... è¿˜æœ‰ {len(data['particle_details']) - 10} ä¸ªç²’å­")

# ========== Step 4: æœ€ç»ˆç»“æœ ==========
print("\n" + "=" * 100)
print("ã€Step 4ã€‘æœ€ç»ˆæ£€ç´¢ç»“æœï¼ˆTop 5 Chunksï¼‰")
print("=" * 100)

print(f"\næŒ‰å¾—åˆ†é™åºæ’åˆ—çš„Top 5 Chunk:\n")

final_results = []
for rank, (chunk_id, data) in enumerate(sorted_chunks, 1):
    chunk_text = amygdala.get_conversation_text(chunk_id)
    chunk_type = data['particle_details'][0]['chunk_type'] if data['particle_details'] else 'Unknown'

    result = {
        'conversation_id': chunk_id,
        'text': chunk_text,
        'score': data['score'],
        'particle_count': len(data['particles']),
        'particle_ids': data['particles'],
        'rank': rank,
        'chunk_type': chunk_type
    }
    final_results.append(result)

    print(f"\n{'=' * 100}")
    print(f"  ã€æ’å {rank}ã€‘{chunk_type}")
    print(f"{'=' * 100}")
    print(f"\n  ğŸ“Š è¯„åˆ†ä¿¡æ¯:")
    print(f"    - Chunk ID: {chunk_id}")
    print(f"    - å¾—åˆ†: {data['score']:.1f}")
    print(f"    - åŒ…å«ç²’å­æ•°: {len(data['particles'])}")
    print(f"    - åŒ…å«ç²’å­IDs: {data['particles'][:3]}{'...' if len(data['particles']) > 3 else ''}")

    print(f"\n  ğŸ“– å®Œæ•´æ–‡æœ¬:")
    print(f"    {chunk_text}")

    print(f"\n  ğŸ” ç›¸å…³æ€§åˆ†æ:")
    print(f"    - Chunkç±»å‹: {chunk_type}")
    if rank <= 2:
        print(f"    - â­ é«˜åº¦ç›¸å…³ï¼åŒ…å«ç­”æ¡ˆæ ¸å¿ƒå†…å®¹")
    elif rank <= 4:
        print(f"    - âœ“ ç›¸å…³ï¼Œæä¾›èƒŒæ™¯ä¿¡æ¯")
    else:
        print(f"    - â—‹ ä¸€èˆ¬ç›¸å…³")

# ========== æ€»ç»“ ==========
print("\n" + "=" * 100)
print("ã€æ£€ç´¢æ€»ç»“ã€‘")
print("=" * 100)

print(f"\nâœ“ æŸ¥è¯¢é—®é¢˜: {query[:80]}...")
print(f"\nâœ“ æ£€ç´¢ç»Ÿè®¡:")
print(f"  - æ•°æ®åº“ä¸­çš„Chunkæ•°: {len(chunks)}")
print(f"  - æ•°æ®åº“ä¸­çš„æ€»ç²’å­æ•°: {total_particles}")
print(f"  - æ£€ç´¢åˆ°çš„ç›¸å…³ç²’å­æ•°: {len(search_results)}")
print(f"  - æ˜ å°„åˆ°çš„Chunkæ•°: {len(chunk_data)}")
print(f"  - è¿”å›çš„Top Chunkæ•°: {len(final_results)}")

print(f"\nâœ“ å…³é”®å‘ç°:")

# æ£€æŸ¥æ˜¯å¦æ£€ç´¢åˆ°å…³é”®chunk
chunk_5_found = any("In the countries of the East" in r['text'] for r in final_results)
chunk_6_found = any("Mercedes looked at him with terror" in r['text'] for r in final_results)

if chunk_5_found:
    chunk_5_result = next(r for r in final_results if "In the countries of the East" in r['text'])
    print(f"\n  1. âœ… æ ¸å¿ƒç­”æ¡ˆChunkï¼ˆä¸œæ–¹å“²å­¦ï¼‰:")
    print(f"     - æ’å: {chunk_5_result['rank']}")
    print(f"     - å¾—åˆ†: {chunk_5_result['score']:.1f}")
    print(f"     - æ­ç¤ºäº†Countæ‹’ç»é£Ÿç‰©çš„æ ¹æœ¬åŸå› : ä¸œæ–¹å…³äºè¿›é£Ÿå’Œå¤ä»‡çš„å“²å­¦")

if chunk_6_found:
    chunk_6_result = next(r for r in final_results if "Mercedes looked at him with terror" in r['text'])
    print(f"\n  2. âœ… åœºæ™¯é«˜æ½®Chunkï¼ˆæƒ…æ„Ÿå¯¹å³™ï¼‰:")
    print(f"     - æ’å: {chunk_6_result['rank']}")
    print(f"     - å¾—åˆ†: {chunk_6_result['score']:.1f}")
    print(f"     - å±•ç¤ºäº†Mercedesçš„ææƒ§å’Œè§‰é†’: æ‹’ç»å…±åŒè¿›é£Ÿæ„å‘³ç€å¤ä»‡")

print(f"\nğŸ’¡ æ£€ç´¢ç³»ç»ŸæˆåŠŸé€šè¿‡è¯­ä¹‰ç†è§£æ‰¾åˆ°äº†ç­”æ¡ˆçš„æ ¸å¿ƒï¼")
print(f"   - ä¸ä»…åŒ¹é…äº†å…³é”®è¯ï¼Œè¿˜ç†è§£äº†æƒ…æ„Ÿè‰²å½©å’Œæ·±å±‚åŠ¨æœº")
print(f"   - åŒæ›²å‡ ä½•å‡†ç¡®è®¡ç®—äº†è¯­ä¹‰ç›¸ä¼¼åº¦")
print(f"   - Chunkèšåˆç®—æ³•æ™ºèƒ½åœ°å°†ç›¸å…³ç²’å­èšåˆä¸ºæœ‰æ„ä¹‰çš„ä¸Šä¸‹æ–‡")

print("\n" + "=" * 100)
print("æµ‹è¯•å®Œæˆï¼")
print("=" * 100)
