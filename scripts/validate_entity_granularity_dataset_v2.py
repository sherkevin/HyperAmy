#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
éªŒè¯å®ä½“ç²’åº¦æ•°æ®é›†v2çš„è´¨é‡

æ£€æŸ¥é¡¹ï¼š
1. æ•°æ®æ ¼å¼æ­£ç¡®æ€§
2. soft_labelä¸å½’ä¸€åŒ–ï¼ˆå’Œä¸ä¸€å®š=1.0ï¼‰
3. intensityä½¿ç”¨L2-norm
4. ç« èŠ‚æ ‡é¢˜è¿‡æ»¤ï¼ˆä¸åº”åŒ…å«å¤§é‡Chapterï¼‰
5. æ–‡æœ¬è´¨é‡ï¼ˆé•¿åº¦ã€å†…å®¹ï¼‰
6. å®ä½“è´¨é‡ï¼ˆä½ç½®åŒ¹é…ã€æ•°é‡ï¼‰
7. QAå¯¹æ•°æ®ï¼ˆå¦‚æœåŒ…å«ï¼‰
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def is_chapter_title(text: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜"""
    text_clean = text.strip()
    if len(text_clean) < 50 and re.search(r'\bChapter\s+\d+', text_clean, re.IGNORECASE):
        return True
    lines = text_clean.split('\n')
    if len(lines) <= 2:
        for line in lines:
            if re.search(r'\bChapter\s+\d+', line, re.IGNORECASE):
                return True
    return False


def validate_dataset_v2(dataset_file: Path) -> Dict[str, Any]:
    """
    éªŒè¯æ•°æ®é›†v2çš„è´¨é‡
    
    Returns:
        éªŒè¯ç»“æœå­—å…¸
    """
    results = {
        'total_samples': 0,
        'valid_samples': 0,
        'invalid_samples': 0,
        'total_entities': 0,
        'errors': [],
        'warnings': [],
        'statistics': {
            'text_lengths': [],
            'entities_per_sample': [],
            'intensities': [],
            'soft_label_sums': [],
            'chapter_title_count': 0,
            'avg_entities_per_sample': 0,
            'avg_text_length': 0,
            'avg_intensity': 0,
        }
    }
    
    if not dataset_file.exists():
        results['errors'].append(f"æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
        return results
    
    with open(dataset_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, start=1):
            if not line.strip():
                continue
            
            results['total_samples'] += 1
            
            try:
                data = json.loads(line)
                
                # éªŒè¯å¿…éœ€å­—æ®µ
                if 'text' not in data:
                    results['errors'].append(f"è¡Œ {line_num}: ç¼ºå°‘ 'text' å­—æ®µ")
                    results['invalid_samples'] += 1
                    continue
                
                if 'targets' not in data:
                    results['errors'].append(f"è¡Œ {line_num}: ç¼ºå°‘ 'targets' å­—æ®µ")
                    results['invalid_samples'] += 1
                    continue
                
                text = data['text']
                targets = data['targets']
                
                if not isinstance(targets, list):
                    results['errors'].append(f"è¡Œ {line_num}: 'targets' å¿…é¡»æ˜¯åˆ—è¡¨")
                    results['invalid_samples'] += 1
                    continue
                
                # æ£€æŸ¥æ–‡æœ¬è´¨é‡
                results['statistics']['text_lengths'].append(len(text))
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜ï¼ˆåº”è¯¥è¢«è¿‡æ»¤ï¼‰
                if is_chapter_title(text):
                    results['statistics']['chapter_title_count'] += 1
                    results['warnings'].append(f"è¡Œ {line_num}: åŒ…å«ç« èŠ‚æ ‡é¢˜ï¼ˆåº”è¯¥è¢«è¿‡æ»¤ï¼‰")
                
                # éªŒè¯æ¯ä¸ªtarget
                sample_entities = 0
                for target_idx, target in enumerate(targets):
                    results['total_entities'] += 1
                    sample_entities += 1
                    
                    # éªŒè¯å¿…éœ€å­—æ®µ
                    required_fields = ['span_text', 'char_start', 'char_end', 'soft_label', 'intensity']
                    for field in required_fields:
                        if field not in target:
                            results['errors'].append(
                                f"è¡Œ {line_num}, å®ä½“ {target_idx}: ç¼ºå°‘ '{field}' å­—æ®µ"
                            )
                            results['invalid_samples'] += 1
                            break
                    else:
                        # éªŒè¯å­—ç¬¦ä½ç½®
                        char_start = target['char_start']
                        char_end = target['char_end']
                        
                        if not isinstance(char_start, int) or not isinstance(char_end, int):
                            results['errors'].append(
                                f"è¡Œ {line_num}, å®ä½“ {target_idx}: char_start/char_end å¿…é¡»æ˜¯æ•´æ•°"
                            )
                            results['invalid_samples'] += 1
                            continue
                        
                        if char_start >= char_end:
                            results['errors'].append(
                                f"è¡Œ {line_num}, å®ä½“ {target_idx}: char_start ({char_start}) >= char_end ({char_end})"
                            )
                            results['invalid_samples'] += 1
                            continue
                        
                        # éªŒè¯span_textä¸æ–‡æœ¬ä½ç½®åŒ¹é…
                        span_text = target['span_text']
                        if char_end > len(text):
                            results['errors'].append(
                                f"è¡Œ {line_num}, å®ä½“ {target_idx}: char_end ({char_end}) > æ–‡æœ¬é•¿åº¦ ({len(text)})"
                            )
                            results['invalid_samples'] += 1
                            continue
                        
                        actual_span = text[char_start:char_end]
                        if actual_span.strip() != span_text.strip():
                            results['warnings'].append(
                                f"è¡Œ {line_num}, å®ä½“ {target_idx}: span_text ä¸å®Œå…¨åŒ¹é… "
                                f"(æœŸæœ›: '{actual_span[:30]}', å®é™…: '{span_text[:30]}')"
                            )
                        
                        # éªŒè¯soft_labelç»´åº¦
                        soft_label = target['soft_label']
                        if not isinstance(soft_label, list):
                            results['errors'].append(
                                f"è¡Œ {line_num}, å®ä½“ {target_idx}: soft_label å¿…é¡»æ˜¯åˆ—è¡¨"
                            )
                            results['invalid_samples'] += 1
                            continue
                        
                        if len(soft_label) != 28:
                            results['errors'].append(
                                f"è¡Œ {line_num}, å®ä½“ {target_idx}: soft_label ç»´åº¦ä¸æ˜¯28 ({len(soft_label)})"
                            )
                            results['invalid_samples'] += 1
                            continue
                        
                        # éªŒè¯soft_labelèŒƒå›´ï¼ˆåº”è¯¥åœ¨[0, 1]ï¼‰
                        if soft_label:
                            min_val = min(soft_label)
                            max_val = max(soft_label)
                            if min_val < 0 or max_val > 1:
                                results['errors'].append(
                                    f"è¡Œ {line_num}, å®ä½“ {target_idx}: soft_label è¶…å‡º[0,1]èŒƒå›´ "
                                    f"([{min_val:.4f}, {max_val:.4f}])"
                                )
                                results['invalid_samples'] += 1
                                continue
                            
                            # è®°å½•soft_labelçš„å’Œï¼ˆv2ä¸å½’ä¸€åŒ–ï¼Œå’Œä¸ä¸€å®š=1.0ï¼‰
                            soft_label_sum = sum(soft_label)
                            results['statistics']['soft_label_sums'].append(soft_label_sum)
                            
                            # å¦‚æœå’Œæ¥è¿‘1.0ï¼Œå¯èƒ½æ˜¯æ—§æ ¼å¼ï¼Œç»™å‡ºè­¦å‘Š
                            if abs(soft_label_sum - 1.0) < 0.01:
                                results['warnings'].append(
                                    f"è¡Œ {line_num}, å®ä½“ {target_idx}: soft_label å’Œæ¥è¿‘1.0 ({soft_label_sum:.4f})ï¼Œ"
                                    f"å¯èƒ½æ˜¯æ—§æ ¼å¼ï¼ˆv2ä¸å½’ä¸€åŒ–ï¼‰"
                                )
                        
                        # éªŒè¯intensityè®¡ç®—ï¼ˆv2ä½¿ç”¨L2-normï¼‰
                        intensity = target['intensity']
                        if intensity < 0:
                            results['errors'].append(
                                f"è¡Œ {line_num}, å®ä½“ {target_idx}: intensity æ— æ•ˆ ({intensity})"
                            )
                            results['invalid_samples'] += 1
                            continue
                        
                        # æ£€æŸ¥intensityæ˜¯å¦ç­‰äºL2-normï¼ˆv2æ ¼å¼ï¼‰
                        if soft_label and len(soft_label) == 28:
                            expected_intensity = np.linalg.norm(soft_label)
                            if abs(intensity - expected_intensity) > 0.001:
                                results['errors'].append(
                                    f"è¡Œ {line_num}, å®ä½“ {target_idx}: intensity è®¡ç®—é”™è¯¯ "
                                    f"(æœŸæœ›L2-norm: {expected_intensity:.6f}, å®é™…: {intensity:.6f})"
                                )
                                results['invalid_samples'] += 1
                                continue
                            
                            results['statistics']['intensities'].append(intensity)
                
                results['statistics']['entities_per_sample'].append(sample_entities)
                results['valid_samples'] += 1
                
            except json.JSONDecodeError as e:
                results['errors'].append(f"è¡Œ {line_num}: JSONè§£æå¤±è´¥: {e}")
                results['invalid_samples'] += 1
            except Exception as e:
                results['errors'].append(f"è¡Œ {line_num}: éªŒè¯å¤±è´¥: {e}")
                results['invalid_samples'] += 1
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    if results['statistics']['text_lengths']:
        results['statistics']['avg_text_length'] = sum(results['statistics']['text_lengths']) / len(results['statistics']['text_lengths'])
    if results['statistics']['entities_per_sample']:
        results['statistics']['avg_entities_per_sample'] = sum(results['statistics']['entities_per_sample']) / len(results['statistics']['entities_per_sample'])
    if results['statistics']['intensities']:
        results['statistics']['avg_intensity'] = sum(results['statistics']['intensities']) / len(results['statistics']['intensities'])
    
    return results


def print_validation_results(results: Dict[str, Any]):
    """æ‰“å°éªŒè¯ç»“æœ"""
    print("=" * 80)
    print("å®ä½“ç²’åº¦æ•°æ®é›†v2è´¨é‡éªŒè¯ç»“æœ")
    print("=" * 80)
    print()
    print(f"æ€»æ ·æœ¬æ•°: {results['total_samples']}")
    print(f"æœ‰æ•ˆæ ·æœ¬æ•°: {results['valid_samples']}")
    print(f"æ— æ•ˆæ ·æœ¬æ•°: {results['invalid_samples']}")
    print(f"æ€»å®ä½“æ•°: {results['total_entities']}")
    print()
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = results['statistics']
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
    if stats['text_lengths']:
        print(f"  å¹³å‡æ–‡æœ¬é•¿åº¦: {stats['avg_text_length']:.1f} å­—ç¬¦")
        print(f"  æ–‡æœ¬é•¿åº¦èŒƒå›´: [{min(stats['text_lengths'])}, {max(stats['text_lengths'])}]")
    if stats['entities_per_sample']:
        print(f"  å¹³å‡å®ä½“æ•°/æ ·æœ¬: {stats['avg_entities_per_sample']:.2f}")
        print(f"  å®ä½“æ•°èŒƒå›´: [{min(stats['entities_per_sample'])}, {max(stats['entities_per_sample'])}]")
    if stats['intensities']:
        print(f"  å¹³å‡intensity: {stats['avg_intensity']:.4f}")
        print(f"  intensityèŒƒå›´: [{min(stats['intensities']):.4f}, {max(stats['intensities']):.4f}]")
    if stats['soft_label_sums']:
        avg_sum = sum(stats['soft_label_sums']) / len(stats['soft_label_sums'])
        min_sum = min(stats['soft_label_sums'])
        max_sum = max(stats['soft_label_sums'])
        print(f"  soft_labelå’Œï¼ˆå¹³å‡ï¼‰: {avg_sum:.4f} (èŒƒå›´: [{min_sum:.4f}, {max_sum:.4f}])")
        print(f"  è¯´æ˜: v2æ ¼å¼ä¸å½’ä¸€åŒ–ï¼Œå’Œä¸ä¸€å®š=1.0 âœ…")
    print(f"  ç« èŠ‚æ ‡é¢˜æ•°é‡: {stats['chapter_title_count']} (åº”è¯¥æ¥è¿‘0) {'âœ…' if stats['chapter_title_count'] < results['total_samples'] * 0.01 else 'âš ï¸'}")
    print()
    
    # é”™è¯¯
    if results['errors']:
        print(f"âŒ é”™è¯¯æ•°: {len(results['errors'])}")
        print("å‰10ä¸ªé”™è¯¯:")
        for error in results['errors'][:10]:
            print(f"  - {error}")
        if len(results['errors']) > 10:
            print(f"  ... è¿˜æœ‰ {len(results['errors']) - 10} ä¸ªé”™è¯¯")
        print()
    else:
        print("âœ… æ— é”™è¯¯")
        print()
    
    # è­¦å‘Š
    if results['warnings']:
        print(f"âš ï¸  è­¦å‘Šæ•°: {len(results['warnings'])}")
        print("å‰10ä¸ªè­¦å‘Š:")
        for warning in results['warnings'][:10]:
            print(f"  - {warning}")
        if len(results['warnings']) > 10:
            print(f"  ... è¿˜æœ‰ {len(results['warnings']) - 10} ä¸ªè­¦å‘Š")
        print()
    else:
        print("âœ… æ— è­¦å‘Š")
        print()
    
    # è´¨é‡è¯„ä¼°
    print("ğŸ¯ è´¨é‡è¯„ä¼°ï¼š")
    quality_score = 0
    max_score = 5
    
    # 1. æ ¼å¼æ­£ç¡®æ€§
    if results['invalid_samples'] == 0:
        print("  âœ… æ ¼å¼æ­£ç¡®æ€§: é€šè¿‡")
        quality_score += 1
    else:
        error_rate = results['invalid_samples'] / results['total_samples']
        if error_rate < 0.01:
            print(f"  âš ï¸  æ ¼å¼æ­£ç¡®æ€§: æœ‰å°‘é‡é”™è¯¯ ({results['invalid_samples']}/{results['total_samples']})")
            quality_score += 0.5
        else:
            print(f"  âŒ æ ¼å¼æ­£ç¡®æ€§: é”™è¯¯ç‡è¿‡é«˜ ({error_rate:.2%})")
    
    # 2. soft_labelä¸å½’ä¸€åŒ–
    if stats['soft_label_sums']:
        avg_sum = sum(stats['soft_label_sums']) / len(stats['soft_label_sums'])
        if abs(avg_sum - 1.0) > 0.1:  # å’Œæ˜æ˜¾ä¸ç­‰äº1.0
            print(f"  âœ… soft_labelä¸å½’ä¸€åŒ–: é€šè¿‡ (å¹³å‡å’Œ={avg_sum:.4f})")
            quality_score += 1
        else:
            print(f"  âš ï¸  soft_labelä¸å½’ä¸€åŒ–: å¹³å‡å’Œæ¥è¿‘1.0 ({avg_sum:.4f})ï¼Œå¯èƒ½æ˜¯æ—§æ ¼å¼")
    
    # 3. intensityä½¿ç”¨L2-norm
    if results['errors']:
        l2_norm_errors = sum(1 for e in results['errors'] if 'intensity' in e.lower() and 'L2-norm' in e.lower())
        if l2_norm_errors == 0:
            print("  âœ… intensityä½¿ç”¨L2-norm: é€šè¿‡")
            quality_score += 1
        else:
            print(f"  âŒ intensityä½¿ç”¨L2-norm: æœ‰ {l2_norm_errors} ä¸ªé”™è¯¯")
    else:
        print("  âœ… intensityä½¿ç”¨L2-norm: é€šè¿‡")
        quality_score += 1
    
    # 4. ç« èŠ‚æ ‡é¢˜è¿‡æ»¤
    chapter_rate = stats['chapter_title_count'] / results['total_samples'] if results['total_samples'] > 0 else 0
    if chapter_rate < 0.01:
        print(f"  âœ… ç« èŠ‚æ ‡é¢˜è¿‡æ»¤: é€šè¿‡ (è¿‡æ»¤ç‡={chapter_rate:.2%})")
        quality_score += 1
    else:
        print(f"  âš ï¸  ç« èŠ‚æ ‡é¢˜è¿‡æ»¤: ä»æœ‰ {stats['chapter_title_count']} ä¸ªç« èŠ‚æ ‡é¢˜ ({chapter_rate:.2%})")
        quality_score += 0.5
    
    # 5. æ•°æ®é‡
    if results['total_samples'] >= 100:
        print(f"  âœ… æ•°æ®é‡: å……è¶³ ({results['total_samples']} æ ·æœ¬)")
        quality_score += 1
    elif results['total_samples'] >= 50:
        print(f"  âš ï¸  æ•°æ®é‡: ä¸­ç­‰ ({results['total_samples']} æ ·æœ¬)")
        quality_score += 0.5
    else:
        print(f"  âŒ æ•°æ®é‡: ä¸è¶³ ({results['total_samples']} æ ·æœ¬)")
    
    print()
    print(f"è´¨é‡å¾—åˆ†: {quality_score}/{max_score} ({quality_score/max_score*100:.0f}%)")
    print("=" * 80)
    
    return quality_score >= 4.0  # è´¨é‡å¾—åˆ†>=4.0è®¤ä¸ºåˆæ ¼


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="éªŒè¯å®ä½“ç²’åº¦æ•°æ®é›†v2è´¨é‡")
    parser.add_argument(
        "--dataset",
        type=str,
        default="data/training/entity_granularity/entity_granularity_v2_full.jsonl",
        help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    dataset_file = Path(project_root) / args.dataset
    
    if not dataset_file.exists():
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
        sys.exit(1)
    
    print(f"éªŒè¯æ•°æ®é›†: {dataset_file}")
    results = validate_dataset_v2(dataset_file)
    is_valid = print_validation_results(results)
    
    # å¦‚æœéªŒè¯å¤±è´¥ï¼Œé€€å‡ºç ä¸º1
    sys.exit(0 if is_valid else 1)
