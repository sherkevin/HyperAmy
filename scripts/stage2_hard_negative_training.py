#!/usr/bin/env python3
"""
äºŒé˜¶æ®µè®­ç»ƒï¼šéš¾ä¾‹å¯¹é½ (Hard Negative Alignment)

ä»QAå¯¹æ„é€ æ•°æ®ï¼Œä½¿ç”¨æ¨¡å‹çš„hidden_stateè¿›è¡Œç›¸ä¼¼åº¦æ£€ç´¢æ’åºï¼Œ
ç­›é€‰ä¸ä¸€è‡´çš„éš¾ä¾‹ï¼Œå¹¶è¿›è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒã€‚
"""

import json
import sys
import argparse
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
from collections import defaultdict
import logging
import warnings

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "emos-master"))

from transformers import AutoTokenizer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_qa_data(qa_file: Path) -> List[Dict[str, Any]]:
    """åŠ è½½QAæ•°æ®"""
    with open(qa_file, 'r', encoding='utf-8') as f:
        qa_data = json.load(f)
    logger.info(f"åŠ è½½äº† {len(qa_data)} ä¸ªQAå¯¹")
    return qa_data


def load_chunks_data(chunks_file: Path) -> Dict[int, str]:
    """åŠ è½½chunksæ•°æ®ï¼Œè¿”å›chunk_idåˆ°æ–‡æœ¬çš„æ˜ å°„"""
    chunk_id_to_text = {}
    with open(chunks_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                chunk = json.loads(line)
                chunk_id = chunk.get('chunk_id') or chunk.get('id')
                text = chunk.get('text') or chunk.get('chunk_text') or chunk.get('input') or chunk.get('content', '')
                if chunk_id is not None and text:
                    chunk_id_to_text[chunk_id] = text
    logger.info(f"åŠ è½½äº† {len(chunk_id_to_text)} ä¸ªchunks")
    return chunk_id_to_text


def load_entity_annotations(entity_file: Path) -> Dict[str, Dict[str, Any]]:
    """
    åŠ è½½å®ä½“ç²’åº¦æ•°æ®é›†ï¼Œè¿”å›textåˆ°å®ä½“æ ‡æ³¨çš„æ˜ å°„
    
    Returns:
        Dict[text_normalized, entity_annotation]
    """
    text_to_entities = {}
    
    if not entity_file.exists():
        logger.warning(f"å®ä½“æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {entity_file}")
        return text_to_entities
    
    with open(entity_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                sample = json.loads(line)
                text = sample.get('text', '').strip()
                targets = sample.get('targets', [])
                
                if text and targets:
                    # æ ‡å‡†åŒ–æ–‡æœ¬ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼ï¼Œç”¨äºåŒ¹é…ï¼‰
                    text_normalized = ' '.join(text.split())
                    text_to_entities[text_normalized] = {
                        'text': text,
                        'targets': targets
                    }
    
    logger.info(f"åŠ è½½äº† {len(text_to_entities)} ä¸ªå®ä½“æ ‡æ³¨æ ·æœ¬")
    return text_to_entities


def normalize_text(text: str) -> str:
    """æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºåŒ¹é…ï¼ˆå»é™¤å¤šä½™ç©ºæ ¼ï¼‰"""
    return ' '.join(text.strip().split())


def get_entity_annotation(text: str, text_to_entities: Dict[str, Dict[str, Any]]) -> Optional[List[Dict[str, Any]]]:
    """è·å–æ–‡æœ¬çš„å®ä½“æ ‡æ³¨"""
    text_norm = normalize_text(text)
    
    # ç²¾ç¡®åŒ¹é…
    if text_norm in text_to_entities:
        return text_to_entities[text_norm]['targets']
    
    # æ¨¡ç³ŠåŒ¹é…ï¼ˆå¦‚æœæ–‡æœ¬åŒ…å«åœ¨å®ä½“æ•°æ®é›†çš„æ–‡æœ¬ä¸­ï¼Œæˆ–ç›¸åï¼‰
    for key, value in text_to_entities.items():
        if text_norm in key or key in text_norm:
            if abs(len(text_norm) - len(key)) / max(len(text_norm), len(key), 1) < 0.1:  # é•¿åº¦å·®å¼‚å°äº10%
                return value['targets']
    
    return None


def construct_qa_contexts_data(
    qa_data: List[Dict[str, Any]],
    chunk_id_to_text: Dict[int, str],
    num_negative_contexts: int = 5
) -> List[Dict[str, Any]]:
    """
    ä»QAå¯¹æ„é€ æ•°æ®ï¼šQ + å¤šä¸ªcontexts
    
    Args:
        qa_data: QAæ•°æ®åˆ—è¡¨
        chunk_id_to_text: chunk_idåˆ°æ–‡æœ¬çš„æ˜ å°„
        num_negative_contexts: è´Ÿæ ·æœ¬contextæ•°é‡
    
    Returns:
        æ„é€ çš„æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
        - question: é—®é¢˜æ–‡æœ¬
        - ground_truth_contexts: [context1, context2, ...] (ground truthé¡ºåº)
        - negative_contexts: [context1, context2, ...] (éšæœºè´Ÿæ ·æœ¬)
        - qa_id: QAå¯¹çš„ID
    """
    constructed_data = []
    
    all_chunk_ids = list(chunk_id_to_text.keys())
    
    for qa in tqdm(qa_data, desc="æ„é€ æ•°æ®"):
        question = qa.get('question', '')
        gt_chunk_id = qa.get('chunk_id')
        
        if not question or gt_chunk_id is None:
            continue
        
        gt_context = chunk_id_to_text.get(gt_chunk_id)
        if not gt_context:
            continue
        
        # Ground truth contexts (è¿™é‡Œåªæœ‰ä¸€ä¸ªï¼Œåç»­å¯ä»¥æ‰©å±•)
        ground_truth_contexts = [gt_context]
        
        # éšæœºé€‰æ‹©è´Ÿæ ·æœ¬contexts
        negative_chunk_ids = [cid for cid in all_chunk_ids if cid != gt_chunk_id]
        if len(negative_chunk_ids) < num_negative_contexts:
            num_negative = len(negative_chunk_ids)
        else:
            num_negative = num_negative_contexts
        
        if num_negative > 0:
            selected_negative_ids = np.random.choice(
                negative_chunk_ids,
                size=num_negative,
                replace=False
            ).tolist()
            negative_contexts = [chunk_id_to_text[cid] for cid in selected_negative_ids]
        else:
            negative_contexts = []
        
        constructed_data.append({
            'question': question,
            'ground_truth_contexts': ground_truth_contexts,
            'negative_contexts': negative_contexts,
            'qa_id': qa.get('chunk_id'),  # ä½¿ç”¨chunk_idä½œä¸ºQA ID
            'gt_chunk_id': gt_chunk_id
        })
    
    logger.info(f"æ„é€ äº† {len(constructed_data)} ä¸ªæ•°æ®æ ·æœ¬")
    return constructed_data


def get_token_level_hidden_states(
    model, tokenizer, text: str, device: str = "cpu", max_length: int = 128
) -> torch.Tensor:
    """
    è·å–æ–‡æœ¬çš„token-level hidden states (64ç»´å‘é‡)
    
    Args:
        model: ProbabilisticGBERTV4æ¨¡å‹
        tokenizer: tokenizer
        text: è¾“å…¥æ–‡æœ¬
        device: è®¾å¤‡
        max_length: æœ€å¤§é•¿åº¦
    
    Returns:
        token_vectors: (L, 64) tokençº§åˆ«çš„hidden states
        encoding: tokenizer encodingç»“æœï¼ˆåŒ…å«offset_mappingç­‰ï¼‰
    """
    # Tokenize
    encoding = tokenizer(
        text,
        max_length=max_length,
        padding="max_length",
        truncation=True,
        return_tensors="pt",
        return_offsets_mapping=True,
    )
    
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)
    
    # Forward pass to get token vectors
    with torch.no_grad():
        # è·å–backboneçš„è¾“å‡º
        outputs = model.backbone(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden = outputs.last_hidden_state  # (1, L, 768)
        
        # Project to 64d (Branch A - semantic_head)
        token_vectors = model.semantic_head(last_hidden)  # (1, L, 64)
        token_vectors = token_vectors.squeeze(0)  # (L, 64)
    
    return token_vectors, encoding


def extract_entity_tokens(
    entity: Dict[str, Any],
    encoding: Dict,
    text: str
) -> Optional[torch.Tensor]:
    """
    ä»token-level hidden statesä¸­æå–å®ä½“å¯¹åº”çš„tokenå‘é‡
    
    Args:
        entity: å®ä½“å­—å…¸ï¼ŒåŒ…å«span_text, char_start, char_end
        encoding: tokenizer encodingç»“æœ
        token_vectors: (L, 64) token vectors
        text: åŸå§‹æ–‡æœ¬
    
    Returns:
        entity_token_mask: (L,) bool tensorï¼ŒTrueè¡¨ç¤ºå±äºè¯¥å®ä½“çš„token
    """
    char_start = entity.get('char_start')
    char_end = entity.get('char_end')
    
    if char_start is None or char_end is None:
        return None
    
    # è·å–token offsets
    offsets = encoding["offset_mapping"]
    if isinstance(offsets, list):
        if len(offsets) > 0 and isinstance(offsets[0], list):
            offsets = offsets[0]
        offsets = torch.tensor(offsets)
    else:
        if offsets.dim() > 2:
            offsets = offsets.squeeze(0)
    
    # è·å–attention mask
    attention_mask = encoding["attention_mask"]
    if isinstance(attention_mask, list):
        if len(attention_mask) > 0 and isinstance(attention_mask[0], list):
            attention_mask = attention_mask[0]
        attention_mask = torch.tensor(attention_mask)
    else:
        if attention_mask.dim() > 1:
            attention_mask = attention_mask.squeeze(0)
    
    # åˆ›å»ºå®ä½“maskï¼štokençš„offsetä¸å®ä½“char rangeé‡å 
    token_starts = offsets[:, 0]
    token_ends = offsets[:, 1]
    
    entity_mask = (token_starts < char_end) & (token_ends > char_start) & attention_mask.bool()
    
    return entity_mask


def compute_entity_vector(
    token_vectors: torch.Tensor,
    entity_mask: torch.Tensor
) -> torch.Tensor:
    """
    ä»token vectorsä¸­æå–å®ä½“å‘é‡ï¼ˆmean poolingï¼‰
    
    Args:
        token_vectors: (L, 64) token vectors
        entity_mask: (L,) bool mask
    
    Returns:
        entity_vector: (64,) entity vector
    """
    if entity_mask.sum() == 0:
        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„tokenï¼Œè¿”å›é›¶å‘é‡
        return torch.zeros(token_vectors.shape[1], device=token_vectors.device)
    
    # Mean pooling over entity tokens
    masked_vectors = token_vectors * entity_mask.unsqueeze(-1).float()
    entity_vector = masked_vectors.sum(dim=0) / entity_mask.sum().float()
    
    return entity_vector


def compute_context_similarity(
    q_token_vectors: torch.Tensor,
    q_encoding: Dict,
    q_entities: List[Dict[str, Any]],
    q_text: str,
    c_token_vectors: torch.Tensor,
    c_encoding: Dict,
    c_entities: List[Dict[str, Any]],
    c_text: str,
    device: str = "cpu"
) -> float:
    """
    è®¡ç®—Qå’ŒContextä¹‹é—´çš„å®ä½“çº§åˆ«å¹³å‡ç›¸ä¼¼åº¦
    
    Args:
        q_token_vectors: (L_q, 64) Qçš„token vectors
        q_encoding: Qçš„tokenizer encoding
        q_entities: Qçš„å®ä½“åˆ—è¡¨
        q_text: Qçš„åŸå§‹æ–‡æœ¬
        c_token_vectors: (L_c, 64) Contextçš„token vectors
        c_encoding: Contextçš„tokenizer encoding
        c_entities: Contextçš„å®ä½“åˆ—è¡¨
        c_text: Contextçš„åŸå§‹æ–‡æœ¬
        device: è®¾å¤‡
    
    Returns:
        å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°
    """
    if not q_entities or not c_entities:
        # å¦‚æœæ²¡æœ‰å®ä½“ï¼Œä½¿ç”¨å¥å­çº§åˆ«çš„ç›¸ä¼¼åº¦ï¼ˆmean poolingï¼‰
        q_vec = q_token_vectors.mean(dim=0)
        c_vec = c_token_vectors.mean(dim=0)
        similarity = F.cosine_similarity(q_vec.unsqueeze(0), c_vec.unsqueeze(0))
        return similarity.item()
    
    # æå–Qçš„æ‰€æœ‰å®ä½“å‘é‡
    q_entity_vectors = []
    for entity in q_entities:
        entity_mask = extract_entity_tokens(entity, q_encoding, q_text)
        if entity_mask is not None and entity_mask.sum() > 0:
            entity_vec = compute_entity_vector(q_token_vectors, entity_mask)
            q_entity_vectors.append(entity_vec)
    
    # æå–Contextçš„æ‰€æœ‰å®ä½“å‘é‡
    c_entity_vectors = []
    for entity in c_entities:
        entity_mask = extract_entity_tokens(entity, c_encoding, c_text)
        if entity_mask is not None and entity_mask.sum() > 0:
            entity_vec = compute_entity_vector(c_token_vectors, entity_mask)
            c_entity_vectors.append(entity_vec)
    
    if not q_entity_vectors or not c_entity_vectors:
        # Fallback to sentence-level similarity
        q_vec = q_token_vectors.mean(dim=0)
        c_vec = c_token_vectors.mean(dim=0)
        similarity = F.cosine_similarity(q_vec.unsqueeze(0), c_vec.unsqueeze(0))
        return similarity.item()
    
    # è®¡ç®—æ‰€æœ‰å®ä½“å¯¹çš„ç›¸ä¼¼åº¦
    q_entity_tensor = torch.stack(q_entity_vectors)  # (N_q, 64)
    c_entity_tensor = torch.stack(c_entity_vectors)  # (N_c, 64)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ (N_q, N_c)
    similarity_matrix = F.cosine_similarity(
        q_entity_tensor.unsqueeze(1),  # (N_q, 1, 64)
        c_entity_tensor.unsqueeze(0),  # (1, N_c, 64)
        dim=2
    )
    
    # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
    avg_similarity = similarity_matrix.mean().item()
    
    return avg_similarity


def evaluate_ranking_consistency(
    predict_order: List[int],
    ground_truth_order: List[int]
) -> Dict[str, float]:
    """
    è¯„ä¼°é¢„æµ‹æ’åºå’ŒçœŸå®æ’åºçš„ä¸€è‡´æ€§
    
    Args:
        predict_order: é¢„æµ‹çš„contextç´¢å¼•æ’åºï¼ˆæŒ‰ç›¸ä¼¼åº¦ä»é«˜åˆ°ä½ï¼‰
        ground_truth_order: çœŸå®çš„contextç´¢å¼•æ’åº
    
    Returns:
        ä¸€è‡´æ€§æŒ‡æ ‡å­—å…¸
    """
    # å¦‚æœåªæœ‰ä¸€ä¸ªground truth context
    if len(ground_truth_order) == 1:
        gt_idx = ground_truth_order[0]
        if gt_idx in predict_order:
            position = predict_order.index(gt_idx) + 1  # 1-indexed
            return {
                'gt_position': position,
                'in_top_1': position == 1,
                'in_top_3': position <= 3,
                'in_top_5': position <= 5,
                'kendall_tau': None  # ä¸é€‚ç”¨
            }
        else:
            return {
                'gt_position': len(predict_order) + 1,
                'in_top_1': False,
                'in_top_3': False,
                'in_top_5': False,
                'kendall_tau': None
            }
    
    # å¤šä¸ªground truth contextsï¼šä½¿ç”¨Kendall's Tau
    from scipy.stats import kendalltau
    
    # åˆ›å»ºä½ç½®æ˜ å°„
    predict_ranks = {idx: rank for rank, idx in enumerate(predict_order)}
    gt_ranks = {idx: rank for rank, idx in enumerate(ground_truth_order)}
    
    # åªè€ƒè™‘åŒæ—¶åœ¨ä¸¤ä¸ªæ’åºä¸­çš„contexts
    common_indices = set(predict_order) & set(ground_truth_order)
    if len(common_indices) < 2:
        return {
            'gt_position': None,
            'in_top_1': False,
            'in_top_3': False,
            'in_top_5': False,
            'kendall_tau': 0.0
        }
    
    common_indices_list = list(common_indices)
    predict_rank_list = [predict_ranks[idx] for idx in common_indices_list]
    gt_rank_list = [gt_ranks[idx] for idx in common_indices_list]
    
    tau, p_value = kendalltau(predict_rank_list, gt_rank_list)
    
    return {
        'gt_position': None,
        'in_top_1': None,
        'in_top_3': None,
        'in_top_5': None,
        'kendall_tau': tau if not np.isnan(tau) else 0.0,
        'p_value': p_value
    }


def main():
    parser = argparse.ArgumentParser(description="äºŒé˜¶æ®µè®­ç»ƒï¼šéš¾ä¾‹å¯¹é½")
    parser.add_argument(
        "--qa_file",
        type=str,
        default="data/benchmarks/instinct_qa.json",
        help="QAæ•°æ®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--chunks_file",
        type=str,
        default="data/processed/got_amygdala.jsonl",
        help="Chunksæ•°æ®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--entity_file",
        type=str,
        default="data/training/entity_granularity/entity_granularity_v2_full.jsonl",
        help="å®ä½“æ ‡æ³¨æ•°æ®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--model_checkpoint",
        type=str,
        default="~/Desktop/best_model.pt",
        help="æ¨¡å‹checkpointè·¯å¾„"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="outputs/stage2_training",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        help="è®¾å¤‡ (cpu/cuda)"
    )
    parser.add_argument(
        "--num_negative_contexts",
        type=int,
        default=5,
        help="æ¯ä¸ªQAå¯¹çš„è´Ÿæ ·æœ¬contextæ•°é‡"
    )
    parser.add_argument(
        "--hard_negative_threshold",
        type=float,
        default=3,
        help="éš¾ä¾‹é˜ˆå€¼ï¼ˆground truth contextä¸åœ¨top-Kä¸­ï¼‰"
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="æœ€å¤§å¤„ç†æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("=" * 70)
    logger.info("äºŒé˜¶æ®µè®­ç»ƒï¼šéš¾ä¾‹å¯¹é½ - æ•°æ®æ„é€ å’Œè¯„ä¼°")
    logger.info("=" * 70)
    
    # åŠ è½½æ•°æ®
    logger.info("\nã€é˜¶æ®µ1ã€‘åŠ è½½æ•°æ®...")
    qa_data = load_qa_data(Path(args.qa_file))
    chunk_id_to_text = load_chunks_data(Path(args.chunks_file))
    text_to_entities = load_entity_annotations(Path(args.entity_file))
    
    # æ„é€ æ•°æ®
    logger.info("\nã€é˜¶æ®µ2ã€‘æ„é€ Q+Contextsæ•°æ®...")
    constructed_data = construct_qa_contexts_data(
        qa_data, 
        chunk_id_to_text,
        num_negative_contexts=args.num_negative_contexts
    )
    
    if args.max_samples:
        constructed_data = constructed_data[:args.max_samples]
        logger.info(f"é™åˆ¶å¤„ç†æ ·æœ¬æ•°ä¸º: {args.max_samples}")
    
    # ä¿å­˜æ„é€ çš„æ•°æ®
    constructed_data_file = output_dir / "constructed_data.jsonl"
    with open(constructed_data_file, 'w', encoding='utf-8') as f:
        for item in constructed_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    logger.info(f"ä¿å­˜æ„é€ çš„æ•°æ®åˆ°: {constructed_data_file}")
    
    logger.info(f"\nâœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
    logger.info(f"   æ„é€ äº† {len(constructed_data)} ä¸ªæ ·æœ¬")
    logger.info(f"   æ¯ä¸ªæ ·æœ¬åŒ…å« 1 ä¸ªground truth context å’Œ {args.num_negative_contexts} ä¸ªè´Ÿæ ·æœ¬contexts")
    
    # å±•å¼€æ¨¡å‹checkpointè·¯å¾„
    model_checkpoint_path = Path(args.model_checkpoint).expanduser()
    if not model_checkpoint_path.exists():
        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_checkpoint_path}")
        return
    
    # é˜¶æ®µ3ï¼šåŠ è½½æ¨¡å‹å¹¶æå–hidden states
    logger.info("\nã€é˜¶æ®µ3ã€‘åŠ è½½æ¨¡å‹...")
    try:
        # å°è¯•å¤šç§è·¯å¾„å¯¼å…¥
        GbertPredictor = None
        for emos_dir_name in ["emos", "emos-master"]:
            emos_path = project_root / emos_dir_name
            if emos_path.exists() and (emos_path / "src" / "model.py").exists():
                sys.path.insert(0, str(emos_path))
                try:
                    from src.model import GbertPredictor
                    logger.info(f"âœ… ä» {emos_dir_name} å¯¼å…¥æ¨¡å‹æˆåŠŸ")
                    break
                except ImportError as e:
                    continue
        
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡æŒ‡å®šçš„è·¯å¾„
        if GbertPredictor is None:
            import os
            emos_env_path = os.environ.get('EMOS_PATH', '')
            if emos_env_path and os.path.exists(emos_env_path):
                sys.path.insert(0, emos_env_path)
                try:
                    from src.model import GbertPredictor
                    logger.info(f"âœ… ä»ç¯å¢ƒå˜é‡ EMOS_PATH={emos_env_path} å¯¼å…¥æ¨¡å‹æˆåŠŸ")
                except ImportError:
                    pass
        
        if GbertPredictor is None:
            raise ImportError(f"æ— æ³•æ‰¾åˆ°emosé¡¹ç›®çš„src.modelæ¨¡å—ã€‚å°è¯•çš„è·¯å¾„: {[str(p / 'src' / 'model.py') for p in [project_root / 'emos', project_root / 'emos-master'] if (p / 'src' / 'model.py').exists()]}")
        
        predictor = GbertPredictor.from_checkpoint(
            checkpoint_path=str(model_checkpoint_path),
            model_name="roberta-base",
            device=args.device
        )
        model = predictor.model
        tokenizer = predictor.tokenizer
        logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # é˜¶æ®µ4ï¼šå®ä½“æ ‡æ³¨åŒ¹é…ã€ç›¸ä¼¼åº¦è®¡ç®—å’Œæ’åº
    logger.info("\nã€é˜¶æ®µ4ã€‘æå–hidden stateså’Œè®¡ç®—ç›¸ä¼¼åº¦...")
    evaluation_results = []
    
    for idx, data_item in enumerate(tqdm(constructed_data, desc="å¤„ç†æ ·æœ¬")):
        question = data_item['question']
        gt_contexts = data_item['ground_truth_contexts']
        negative_contexts = data_item['negative_contexts']
        qa_id = data_item['qa_id']
        
        # åˆå¹¶æ‰€æœ‰contextsï¼ˆground truth + negativeï¼‰
        all_contexts = gt_contexts + negative_contexts
        context_indices = list(range(len(all_contexts)))  # 0=gt, 1-N=negative
        
        # æå–Qçš„hidden stateså’Œå®ä½“
        try:
            q_token_vectors, q_encoding = get_token_level_hidden_states(
                model, tokenizer, question, device=args.device
            )
            q_entities = get_entity_annotation(question, text_to_entities)
            if q_entities is None:
                q_entities = []
        except Exception as e:
            logger.warning(f"å¤„ç†Qå¤±è´¥ (QA {qa_id}): {e}")
            continue
        
        # è®¡ç®—æ¯ä¸ªcontextçš„ç›¸ä¼¼åº¦
        context_similarities = []
        context_entities_list = []
        
        for ctx_idx, context_text in enumerate(all_contexts):
            try:
                # æå–contextçš„hidden stateså’Œå®ä½“
                c_token_vectors, c_encoding = get_token_level_hidden_states(
                    model, tokenizer, context_text, device=args.device
                )
                c_entities = get_entity_annotation(context_text, text_to_entities)
                if c_entities is None:
                    c_entities = []
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = compute_context_similarity(
                    q_token_vectors, q_encoding, q_entities, question,
                    c_token_vectors, c_encoding, c_entities, context_text,
                    device=args.device
                )
                
                context_similarities.append((ctx_idx, similarity))
                context_entities_list.append(c_entities)
            except Exception as e:
                logger.warning(f"å¤„ç†Context {ctx_idx}å¤±è´¥ (QA {qa_id}): {e}")
                context_similarities.append((ctx_idx, -1.0))  # å¤±è´¥æ—¶è®¾ä¸º-1
                context_entities_list.append([])
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        context_similarities.sort(key=lambda x: x[1], reverse=True)
        predict_order = [idx for idx, _ in context_similarities]
        
        # Ground truth order (åªæœ‰ç¬¬ä¸€ä¸ªæ˜¯ground truth)
        ground_truth_order = [0]  # ç¬¬ä¸€ä¸ªcontextæ˜¯ground truth
        
        # è¯„ä¼°ä¸€è‡´æ€§
        consistency_metrics = evaluate_ranking_consistency(predict_order, ground_truth_order)
        
        evaluation_results.append({
            'qa_id': qa_id,
            'question': question[:100] + '...' if len(question) > 100 else question,
            'predict_order': predict_order,
            'ground_truth_order': ground_truth_order,
            'similarities': {idx: sim for idx, sim in context_similarities},
            'consistency_metrics': consistency_metrics,
            'q_entities_count': len(q_entities),
            'c_entities_count': [len(ents) for ents in context_entities_list]
        })
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    evaluation_file = output_dir / "evaluation_results.json"
    with open(evaluation_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
    logger.info(f"ä¿å­˜è¯„ä¼°ç»“æœåˆ°: {evaluation_file}")
    
    # é˜¶æ®µ5ï¼šç­›é€‰éš¾ä¾‹
    logger.info("\nã€é˜¶æ®µ5ã€‘ç­›é€‰éš¾ä¾‹...")
    hard_negatives = []
    
    for eval_result in evaluation_results:
        metrics = eval_result['consistency_metrics']
        # å¦‚æœground truth contextä¸åœ¨top-Kä¸­ï¼Œæ ‡è®°ä¸ºéš¾ä¾‹
        if metrics.get('gt_position') and metrics['gt_position'] > args.hard_negative_threshold:
            # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ•°æ®
            qa_id = eval_result['qa_id']
            original_data = next((d for d in constructed_data if d['qa_id'] == qa_id), None)
            if original_data:
                hard_negatives.append(original_data)
    
    logger.info(f"ç­›é€‰å‡º {len(hard_negatives)} ä¸ªéš¾ä¾‹ï¼ˆå…± {len(constructed_data)} ä¸ªæ ·æœ¬ï¼‰")
    
    # ä¿å­˜éš¾ä¾‹
    hard_negatives_file = output_dir / "hard_negatives.jsonl"
    with open(hard_negatives_file, 'w', encoding='utf-8') as f:
        for item in hard_negatives:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    logger.info(f"ä¿å­˜éš¾ä¾‹åˆ°: {hard_negatives_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    logger.info("\n" + "=" * 70)
    logger.info("è¯„ä¼°ç»Ÿè®¡")
    logger.info("=" * 70)
    
    top1_count = sum(1 for r in evaluation_results if r['consistency_metrics'].get('in_top_1'))
    top3_count = sum(1 for r in evaluation_results if r['consistency_metrics'].get('in_top_3'))
    top5_count = sum(1 for r in evaluation_results if r['consistency_metrics'].get('in_top_5'))
    
    logger.info(f"Top-1å‡†ç¡®ç‡: {top1_count}/{len(evaluation_results)} ({100*top1_count/len(evaluation_results):.1f}%)")
    logger.info(f"Top-3å‡†ç¡®ç‡: {top3_count}/{len(evaluation_results)} ({100*top3_count/len(evaluation_results):.1f}%)")
    logger.info(f"Top-5å‡†ç¡®ç‡: {top5_count}/{len(evaluation_results)} ({100*top5_count/len(evaluation_results):.1f}%)")
    logger.info(f"éš¾ä¾‹æ•°é‡: {len(hard_negatives)} ({100*len(hard_negatives)/len(evaluation_results):.1f}%)")
    
    logger.info("\nâœ… è¯„ä¼°å®Œæˆï¼")
    logger.info(f"\nğŸ“ ä¸‹ä¸€æ­¥ï¼šä½¿ç”¨éš¾ä¾‹æ•°æ®è¿›è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒ")
    logger.info(f"   éš¾ä¾‹æ–‡ä»¶: {hard_negatives_file}")


if __name__ == "__main__":
    main()
